{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy-CNN-Rafa",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuanPCunha/TCC/blob/main/Copy_CNN_Rafa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloads e imports"
      ],
      "metadata": {
        "id": "EvS_6l_yMUeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "N5Vhr4_vECld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ptoQwAGEDQnl",
        "outputId": "82953d55-53a4-489f-f293-9670e377048a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "start_time = time.time()\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# load das libs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import schedules \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.backend import dropoutpout\n",
        "#, Activation\n",
        "# from keras.layers import Embedding\n",
        "# from keras.preprocessing import sequence\n",
        "# from keras.layers import Dense, Dro\n",
        "# from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# random state\n",
        "sd = 5\n",
        "np.random.seed(sd)\n",
        "rn.seed(sd)\n",
        "os.environ['PYTHONHASHSEED']=str(sd)\n",
        "tf.random.set_seed(sd)"
      ],
      "metadata": {
        "id": "OeOYrHW9Bxs_",
        "outputId": "e95e5491-564f-451d-cbd0-e42a7320b1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d66f7ec4135b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdropoutpout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#, Activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# from keras.layers import Embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'dropoutpout' from 'keras.backend' (/usr/local/lib/python3.7/dist-packages/keras/backend.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carrega base"
      ],
      "metadata": {
        "id": "kMO-knebMcYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base1 Com StopWords\n",
        "PATH_BASE1 = \"/content/drive/MyDrive/TCC/dados/processadas/Base1_classificada.csv\"\n",
        "\n",
        "# Base2 Sem StopWords\n",
        "PATH_BASE2 = \"/content/drive/MyDrive/TCC/dados/processadas/Base2_classificada.csv\"\n",
        "\n",
        "# Base1 Com StopWords\n",
        "PATH_BASE1_JUNTO_COM_DA_LEILA_BALANCEADA = \"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base1JuntaHateENotHateDaLeila_balanceada.csv\"\n",
        "#MATRIZ_CBOW_300_BASE_1 = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base1JuntaHateENotHateDaLeila_balanceada_CBOW300.CSV\", delimiter=',')\n",
        "\n",
        "# Base2 Sem StopWords\n",
        "PATH_BASE2_LEILA_LIMPISSIMA_BALANCEADA = \"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base2_maior4_menor25_limpissima_balanceada.csv\"\n",
        "#MATRIZ_CBOW_300_BASE_2_LEILA = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base2_maior4_menor25_limpissima_balanceada_CBOW300.CSV\", delimiter=',')\n",
        "\n",
        "PATH_BASE_2_CLASSIFICADA = r\"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base2_classificada_balanceada.csv\" #entrada\n",
        "MATRIZ_CBOW_300_BASE_2 = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base2_classificada_balanceada_CBOW300.csv\", delimiter=',') #saida"
      ],
      "metadata": {
        "id": "xf_v4U5DDfU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(PATH_BASE_2_CLASSIFICADA, index_col=0)\n",
        "reviews"
      ],
      "metadata": {
        "id": "ucr9GfQXBuB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execução modelo"
      ],
      "metadata": {
        "id": "W9pdFxE7Ml6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "texto = reviews['text']\n",
        "texto"
      ],
      "metadata": {
        "id": "7wJXwK7jCkyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "metadata": {
        "id": "db4KNc_2Ixyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(x, length=None):\n",
        "    return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "metadata": {
        "id": "pwcjgZd9I6AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(x):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "\n",
        "    return preprocess_x, x_tk"
      ],
      "metadata": {
        "id": "fNMyJgcsI8AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "pX78imOBI9l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxNdO2PW2E5a"
      },
      "outputs": [],
      "source": [
        "print(reviews.head)\n",
        "\n",
        "text_tokenized, text_tokenizer = tokenize(texto)\n",
        "\n",
        "test_pad = pad(text_tokenized)\n",
        "\n",
        "y = reviews['label']\n",
        "\n",
        "preproc_texto, texto_tokenizer = preprocess(texto)\n",
        "    \n",
        "max_text_length = preproc_texto.shape[1]\n",
        "text_vocab_size = len(texto_tokenizer.word_index)\n",
        "\n",
        "worddx = texto_tokenizer.word_index\n",
        "\n",
        "print(\"Max sentence length:\", max_text_length)\n",
        "print(\"Vocabulary size:\", text_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parametros:\n",
        "tam_vocab = text_vocab_size # vocabulário do embedding\n",
        "tam_max = max_text_length # 100\n",
        "batchsize = 64  # 64\n",
        "embedding_dimen = 300  # 50 ! 300\n",
        "filtros = 250 #250\n",
        "kernel_size = 2 # 3 | 2\n",
        "hidden_dims = 100 # not used\n",
        "epochs = 50\n",
        "lr = 0.0006 # 0.00006\n",
        "drp = 0.4   # coeficiente de dropout\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(test_pad, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "yBGbbGdfL7ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = schedules.ExponentialDecay(\n",
        "    initial_learning_rate=lr,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = Adam(learning_rate=lr_schedule) # optimizer = SGD(learning_rate=lr_schedule)\n",
        "\n",
        "start_time2 = time.time()\n",
        "#wvec = KeyedVectors.load_word2vec_format('/content/cbow_s300.txt')\n",
        "#wvec = KeyedVectors.load_word2vec_format(\"cbow_s50.txt\")\n",
        "#wvec = KeyedVectors.load_word2vec_format(\"skip_s50.txt\")\n",
        "print(); print(\"Time to load model: %.5s s.\\n\" % (time.time() - start_time2))\n",
        "\n",
        "\n",
        "# gerando a matriz do embedding\n",
        "# embedding_matrix = np.zeros((tam_vocab+1, embedding_dimen))\n",
        "# for word, i in worddx.items():\n",
        "#     if i>=tam_vocab:\n",
        "#         continue\n",
        "#     try:\n",
        "#         embedding_vector = wvec[word]\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#     except KeyError:\n",
        "#         embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dimen)\n",
        "\n",
        "# del(wvec)\n",
        "\n",
        "# defininindo a camada de embedding utilizando a matriz\n",
        "# embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "#                             output_dim=embedding_matrix.shape[1],\n",
        "#                             weights=[embedding_matrix],\n",
        "#                             input_length=tam_max,\n",
        "#                             trainable=False)\n",
        "\n",
        "#del(embedding_matrix)\n",
        "\n",
        "#    embedding_vec = embeddings_index.get(word)\n",
        "#    if embedding_vec is not None:\n",
        "#        embedding_matrix[index] = embedding_vec\n",
        "\n",
        "model = Sequential()\n",
        "# primeira camada se trata do embedding. \n",
        "#model.add(embedding_layer)\n",
        "model.add(Embedding(tam_vocab, embedding_dimen, input_length=tam_max)) \n",
        "\n",
        "\n",
        "# input = numero de possiveis palavras, dimensão do espaço(complex!), e 137\n",
        "#model.add(Embedding(tam_vocab, embedding_dimen, input_length=tam_max)) \n",
        "\n",
        "# camada convolucional\n",
        "# filtros referentes ao output dessa camada,\n",
        "# tamanho nucleo = 3, se tratando apenas de uma dimensão (conv1d)\n",
        "# padding 'valid' = sem padding\n",
        "# função de ativação = relu\n",
        "model.add(Dropout(drp))\n",
        "model.add(Conv1D(filtros, kernel_size, padding='same', activation='relu', strides=1))\n",
        "model.add(Conv1D(filtros, kernel_size, padding='same', activation='relu', strides=1))\n",
        "model.add(Dropout(drp))\n",
        "model.add(MaxPooling1D())\n",
        "\n",
        "\n",
        "# camada convolucional                         'valid|same'\n",
        "model.add(Conv1D(filtros, kernel_size, padding='same', activation='relu', strides=1))\n",
        "model.add(Conv1D(filtros, kernel_size, padding='same', activation='relu', strides=1))\n",
        "model.add(Dropout(drp))\n",
        "model.add(MaxPooling1D())\n",
        "\n",
        "\n",
        "# nivela o output para utilização na camada densa\n",
        "model.add(Flatten())\n",
        "#model.add(Dense(hidden_dims, activation='relu'))\n",
        "\n",
        "# regularização por dropout\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(2, activation='sigmoid')) #FUNÇÃO VAGABUNDA\n",
        "model.add(Dense(2, activation='softmax')) #FUNÇÃO VAGABUNDA\n",
        "\n",
        "# calculo de loss adotado = binary_crossentropy/categorical_crossentropy\n",
        "#model.compile(loss='binary_crossentropy', optimizer=Adam(lr), metrics=['accuracy'])\n",
        "#model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=optimizer, metrics=['accuracy']) \n",
        "model.summary()\n",
        "\n",
        "# Fit the model                                                                                               #val split\n",
        "history = model.fit(X_train, y_train, batch_size=batchsize, epochs=epochs,validation_data=(X_test, y_test), validation_split = 0.3, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# tempo de execução total\n",
        "print(); print(\"Tempo de execução: %.5s segundos.\\n\" % (time.time() - start_time))\n",
        "\n",
        "# plot dos gráficos\n",
        "ntl.plot_accuracy(history)\n",
        "ntl.plot_loss(history)"
      ],
      "metadata": {
        "id": "vrC1KA9FLUK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, to_file='/meuarquivo.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "SqzyjKSXJ6Tq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}