{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RotinasDeLimpezaDaBaseDeDados.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuanPCunha/TCC/blob/main/RotinasDeLimpezaDaBaseDeDados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8-4D6gHR9cH",
        "outputId": "ae631df4-e892-4e7f-f377-94153f0fd760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85MWVC-MRkEq"
      },
      "source": [
        "Base de Dados Twitter\n",
        "\n",
        "An√°lise da base de dados do Twitter com linguagem ofensiva\n",
        "\n",
        "Pr√© - Processamento do texto\n",
        "\n",
        "Classifica√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJufh39Rono"
      },
      "source": [
        "Import e instala√ß√£o das bibliotecas necess√°rias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhGx5h16Rj0l",
        "outputId": "85e3a5f5-ca11-4909-e754-4708866b70f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Install e downloads\n",
        "!pip install emoji\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "#nltk.download('all-nltk') #Demora um pouco"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVE3AI4FRwiJ"
      },
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gx9VCYwRzhC"
      },
      "source": [
        "# Defini√ß√£o de constantes\n",
        "\n",
        "# Padr√£o RegEx\n",
        "PADRAO_PALAVRAS_PT = r'[a-zA-Z√†-√∫√Ä-√ö0-9]+'\n",
        "\n",
        "# Quantidade tokens tweets pequeno\n",
        "QUANT_TOKENS_MIN = 5\n",
        "\n",
        "########## PATHS DE ARQUIVOS ##########\n",
        "# Base completa bruta\n",
        "PATH_BASE_COMPLETA_BRUTA = r\"/content/drive/MyDrive/TCC/dados/tweets_coletados/base80k.csv\"\n",
        "\n",
        "# Base teste bruta\n",
        "PATH_BASE_TESTE_BRUTA = r\"/content/drive/MyDrive/TCC/dados/tweets_coletados/base_teste.csv\"\n",
        "\n",
        "# Dicion√°rio de palavr√µes\n",
        "PATH_DICT_PALAVROES = r\"/content/drive/MyDrive/TCC/dados/dicionarios/badword_list.csv\"\n",
        "\n",
        "# Dicion√°rio de internetes\n",
        "PATH_DICT_INTERNETES = r\"/content/drive/MyDrive/TCC/dados/dicionarios/internetes_list.csv\"\n",
        "\n",
        "# Dicion√°rio de estados\n",
        "PATH_DICT_ESTADOS = r\"/content/drive/MyDrive/TCC/dados/dicionarios/estados_list.csv\"\n",
        "\n",
        "# Dicion√°rio de express√µes contra√≠das\n",
        "PATH_DICT_EXPRESSOES = r\"/content/drive/MyDrive/TCC/dados/dicionarios/dict_hate_contraidos.csv\"\n",
        "\n",
        "# Lista de vogais duplicadas\n",
        "PATH_LIST_VOGAIS_DUPLICADAS = r\"/content/drive/MyDrive/TCC/dados/dicionarios/vogais_duplicadas.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaVkPTQ-T5gc",
        "outputId": "cfc32aef-0569-41e7-a786-0572cda388e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Carrega a base de dados\n",
        "baseTweets = pd.read_csv(PATH_BASE_TESTE_BRUTA, index_col=0)\n",
        "baseTweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_str</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>entities.hashtags</th>\n",
              "      <th>place.full_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1037067688953167879</td>\n",
              "      <td>Tue Sep 04 19:59:39 +0000 2018</td>\n",
              "      <td>O famoso: Deus me livre, mas quem me dera... h...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rio de Janeiro, Brazil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101497244325502977</td>\n",
              "      <td>Fri Mar 01 14:59:42 +0000 2019</td>\n",
              "      <td>Eu to ficando maluco ou o Weverton do Palmeira...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Divin√≥polis, Brasil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1085552210191233025</td>\n",
              "      <td>Wed Jan 16 14:59:50 +0000 2019</td>\n",
              "      <td>üëâüèæ Hoje: SAMBINHA DE QUARTA \\r\\nüìçHashtag Bar -...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rio de Janeiro, Brazil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1151869182373482501</td>\n",
              "      <td>Thu Jul 18 14:59:48 +0000 2019</td>\n",
              "      <td>@rick_pcf @ygorcac 2017 de primeira achei feio...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Fortaleza, Brazil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1148607640383217664</td>\n",
              "      <td>Tue Jul 09 14:59:36 +0000 2019</td>\n",
              "      <td>@conexaopolitica √â um bund√£o ....</td>\n",
              "      <td>[]</td>\n",
              "      <td>Bigua√ßu, Brasil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                id_str  ...         place.full_name\n",
              "0  1037067688953167879  ...  Rio de Janeiro, Brazil\n",
              "1  1101497244325502977  ...     Divin√≥polis, Brasil\n",
              "2  1085552210191233025  ...  Rio de Janeiro, Brazil\n",
              "3  1151869182373482501  ...       Fortaleza, Brazil\n",
              "4  1148607640383217664  ...         Bigua√ßu, Brasil\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseTweets.shape"
      ],
      "metadata": {
        "id": "4wkzYMsa_LcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXsmpxj_VCxY"
      },
      "source": [
        "Fase 1\n",
        "Defini√ß√£o das fun√ß√µes usadas na limpeza Etapas:\n",
        "\n",
        "1 Remover men√ß√µes (Remover @fulano)\n",
        "2 Remover caracteres especiais Remover caracteres de pontua√ß√£o\n",
        "3 Remover emojis\n",
        "4 Expandir internet√™s\n",
        "5 Normalizar nomes de estados (SP->S√£o Paulo)\n",
        "6 Retirar letras repetidas\n",
        "7 Remover tweets com <5 tokens\n",
        "8 Remover tweets repetidos (texto id√™ntico)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLDPHRyvVEsD"
      },
      "source": [
        "# Main\n",
        "def pre_processamento(coluna_text):\n",
        "    baseTweets['text_limpo'] = coluna_text # Inicializa nova coluna\n",
        "    indice = 0\n",
        "    tokenizer = RegexpTokenizer(PADRAO_PALAVRAS_PT)\n",
        "    dict_internetes = pd.read_csv(PATH_DICT_INTERNETES)\n",
        "    dict_estados = pd.read_csv(PATH_DICT_ESTADOS)\n",
        "    dict_expressoes = pd.read_csv(PATH_DICT_EXPRESSOES, encoding='latin-1')\n",
        "    list_vogais_duplicadas = pd.read_csv(PATH_LIST_VOGAIS_DUPLICADAS)\n",
        "    for text in coluna_text:\n",
        "        text = remover_mencao(text.lower()) # Remover men√ß√µes\n",
        "        text = re.sub(r'http\\S+', '', text) # Remover links\n",
        "        tokens = tokenizer.tokenize(text) # Remover caracteres especiais, pontua√ß√£o e emojis\n",
        "        tokens = expandir_sigla(tokens,dict_internetes) # Expandir Internetes\n",
        "        tokens = expandir_sigla(tokens,dict_estados) # Normalizar nomes de estado\n",
        "        tokens = remover_letras_repetidas(tokens, list_vogais_duplicadas) # Retirar letras repetidas\n",
        "        tokens = seta_tweet_pequeno_para_um(tokens) # Marca tweets pequenos\n",
        "\n",
        "        text_limpo = \" \".join(tokens)\n",
        "        baseTweets['text_limpo'][indice] = text_limpo\n",
        "\n",
        "        indice+=1 # Incrementa contador\n",
        "\n",
        "    # Remover tweets marcados como pequenos e remove tweets repetidos(texto id√™ntico) \n",
        "    baseTweets.drop_duplicates(subset='text_limpo', keep=False, inplace=True)\n",
        "    #baseTweets.reset_index(inplace=True)\n",
        "\n",
        "    # apaga as linhas com o texto = '1'\n",
        "    indexNames = baseTweets[ baseTweets['text_limpo'] == '1' ].index    \n",
        "    baseTweets.drop(indexNames , inplace=True)\n",
        "\n",
        "\n",
        "# Remover men√ß√µes   \n",
        "def remover_mencao(tweet):\n",
        "    tweet_sem_mencao = re.sub(r'[@]\\w+', '', tweet)\n",
        "    tweet_sem_mencao = \" \".join(tweet_sem_mencao.split()) # Remover espa√ßos a mais\n",
        "    return tweet_sem_mencao\n",
        "\n",
        "# ESSA FUN√á√ÉO √â CASE-SENSITIVE\n",
        "def expandir_sigla(tokens, dicionario):\n",
        "    dicionario_py = dicionario.to_dict() # Converte Dataframe para dicion√°rio python\n",
        "    keys = list(dicionario_py.keys()) # Dicion√°rio de 2 colunas: chave, valor\n",
        "    tokenizer = RegexpTokenizer(PADRAO_PALAVRAS_PT)\n",
        "    indice = 0\n",
        "    for sigla in dicionario_py[keys[0]].values():\n",
        "        if sigla in tokens:\n",
        "            text = \" \".join(tokens) # Agrupa os tokens formando o texto do tweet\n",
        "            significado = dicionario_py[keys[1]][indice]\n",
        "            text = re.sub(r'^' + sigla + '\\s', significado + ' ',text) # Substitui no in√≠cio\n",
        "            text = re.sub(r'\\s' + sigla + '$',' '+ significado,text) # Substitui no final\n",
        "            text = re.sub(r'\\s' + sigla + '\\s', ' '+significado+ ' ',text) #Substitui no meio\n",
        "            tokens = tokenizer.tokenize(text) # Tokeniza o texto do tweet ap√≥s troca\n",
        "            #print(tokens)\n",
        "\n",
        "        indice+=1 # Incremento do contador\n",
        "    return tokens\n",
        "\n",
        "# Retirar letras repetidas\n",
        "def remover_letras_repetidas(tokens, lista):\n",
        "    lista = list(lista['vogais_duplicadas'])\n",
        "    tokens_sem_letras_repetidas = []\n",
        "    for token in tokens:\n",
        "        flag = True\n",
        "        if token not in lista: # Se for uma palavra com vogais dobradas n√£o modifica o token\n",
        "            for i in range(len(token)):\n",
        "                # Se tiver \"rr\" ou \"ss\" n√£o modifica o token\n",
        "                try:\n",
        "                    if (token[i].lower()=='r' and token[i+1].lower()=='r') or\\\n",
        "                        (token[i].lower()=='s' and token[i+1].lower()=='s'):\n",
        "                        flag = False\n",
        "                        break\n",
        "                except:\n",
        "                    break\n",
        "            if flag:\n",
        "                # Remove letras repetidas\n",
        "                token = ''.join(c[0] for c in itertools.groupby(token))\n",
        "        tokens_sem_letras_repetidas.append(token)\n",
        "    return tokens_sem_letras_repetidas\n",
        "\n",
        "# Marca tweets pequenos\n",
        "def seta_tweet_pequeno_para_um(tokens):\n",
        "    if len(tokens)<QUANT_TOKENS_MIN:\n",
        "        tokens = '1'\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUHrn97DYh89",
        "outputId": "63055a4b-ab2a-4179-bc33-4702281ad8d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Aplicar pr√© processamento na base\n",
        "pre_processamento(baseTweets['text'])\n",
        "baseTweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>id_str</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>entities.hashtags</th>\n",
              "      <th>place.full_name</th>\n",
              "      <th>text_limpo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1037067688953167879</td>\n",
              "      <td>Tue Sep 04 19:59:39 +0000 2018</td>\n",
              "      <td>O famoso: Deus me livre, mas quem me dera... h...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rio de Janeiro, Brazil</td>\n",
              "      <td>o famoso deus me livre mas quem me dera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1101497244325502977</td>\n",
              "      <td>Fri Mar 01 14:59:42 +0000 2019</td>\n",
              "      <td>Eu to ficando maluco ou o Weverton do Palmeira...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Divin√≥polis, Brasil</td>\n",
              "      <td>eu estou ficando maluco ou o weverton do palme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1085552210191233025</td>\n",
              "      <td>Wed Jan 16 14:59:50 +0000 2019</td>\n",
              "      <td>üëâüèæ Hoje: SAMBINHA DE QUARTA \\r\\nüìçHashtag Bar -...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rio de Janeiro, Brazil</td>\n",
              "      <td>hoje sambinha de quarta hashtag bar seabra fil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1151869182373482501</td>\n",
              "      <td>Thu Jul 18 14:59:48 +0000 2019</td>\n",
              "      <td>@rick_pcf @ygorcac 2017 de primeira achei feio...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Fortaleza, Brazil</td>\n",
              "      <td>2017 de primeira achei feio talvez por ser mei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1074695634513469440</td>\n",
              "      <td>Mon Dec 17 15:59:40 +0000 2018</td>\n",
              "      <td>P= mt chato mas eu gosto mt‚ù§Ô∏è</td>\n",
              "      <td>[]</td>\n",
              "      <td>Maric√°, Brasil</td>\n",
              "      <td>p muito chato mas eu gosto muito</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                         text_limpo\n",
              "0      0  ...            o famoso deus me livre mas quem me dera\n",
              "1      1  ...  eu estou ficando maluco ou o weverton do palme...\n",
              "2      2  ...  hoje sambinha de quarta hashtag bar seabra fil...\n",
              "3      3  ...  2017 de primeira achei feio talvez por ser mei...\n",
              "4      5  ...                   p muito chato mas eu gosto muito\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseTweets.shape"
      ],
      "metadata": {
        "id": "BB4RWsuA_PPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFZz24V3Fecx"
      },
      "source": [
        "baseTweets.to_csv('Base1_Com_StopWords.csv') #Salva base limpa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTxagMLGFCo_"
      },
      "source": [
        "Fase 2\n",
        "remover stopswords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWB-wfr8YqqZ"
      },
      "source": [
        "# O arquivo \"portuguese\" contem Stopwords em Portugues\n",
        "Sw = open('/content/drive/MyDrive/TCC/dados/dicionarios/portuguese.txt','r',encoding='utf-8')\n",
        "ptBr_stopwords = Sw.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ut0uAnFMjB",
        "outputId": "01990759-046f-4fa7-9330-8d118952f963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "baseTweets['text_cleanStopWords'] = baseTweets['text_limpo'].apply(lambda x: ' '.join([word for word in x.split() if word not in (ptBr_stopwords)]))\n",
        "baseTweets.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>id_str</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>entities.hashtags</th>\n",
              "      <th>place.full_name</th>\n",
              "      <th>text_limpo</th>\n",
              "      <th>text_cleanStopWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1037067688953167879</td>\n",
              "      <td>Tue Sep 04 19:59:39 +0000 2018</td>\n",
              "      <td>O famoso: Deus me livre, mas quem me dera... h...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rio de Janeiro, Brazil</td>\n",
              "      <td>o famoso deus me livre mas quem me dera</td>\n",
              "      <td>famoso deus livre dera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1101497244325502977</td>\n",
              "      <td>Fri Mar 01 14:59:42 +0000 2019</td>\n",
              "      <td>Eu to ficando maluco ou o Weverton do Palmeira...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Divin√≥polis, Brasil</td>\n",
              "      <td>eu estou ficando maluco ou o weverton do palme...</td>\n",
              "      <td>ficando maluco weverton palmeiras convocado se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1085552210191233025</td>\n",
              "      <td>Wed Jan 16 14:59:50 +0000 2019</td>\n",
              "      <td>üëâüèæ Hoje: SAMBINHA DE QUARTA \\r\\nüìçHashtag Bar -...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Rio de Janeiro, Brazil</td>\n",
              "      <td>hoje sambinha de quarta hashtag bar seabra fil...</td>\n",
              "      <td>sambinha quarta hashtag bar seabra filho sexta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1151869182373482501</td>\n",
              "      <td>Thu Jul 18 14:59:48 +0000 2019</td>\n",
              "      <td>@rick_pcf @ygorcac 2017 de primeira achei feio...</td>\n",
              "      <td>[]</td>\n",
              "      <td>Fortaleza, Brazil</td>\n",
              "      <td>2017 de primeira achei feio talvez por ser mei...</td>\n",
              "      <td>2017 achei feio talvez meio diferente demorei ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1074695634513469440</td>\n",
              "      <td>Mon Dec 17 15:59:40 +0000 2018</td>\n",
              "      <td>P= mt chato mas eu gosto mt‚ù§Ô∏è</td>\n",
              "      <td>[]</td>\n",
              "      <td>Maric√°, Brasil</td>\n",
              "      <td>p muito chato mas eu gosto muito</td>\n",
              "      <td>chato gosto</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                text_cleanStopWords\n",
              "0      0  ...                             famoso deus livre dera\n",
              "1      1  ...  ficando maluco weverton palmeiras convocado se...\n",
              "2      2  ...  sambinha quarta hashtag bar seabra filho sexta...\n",
              "3      3  ...  2017 achei feio talvez meio diferente demorei ...\n",
              "4      5  ...                                        chato gosto\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FsVbVhzFNJw"
      },
      "source": [
        "baseTweets.to_csv('Base2_Sem_StopWords.csv') #Salva base limpa"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}