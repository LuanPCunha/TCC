{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Treinamento_BiLSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuanPCunha/TCC/blob/main/Treinamento_BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install keras"
      ],
      "metadata": {
        "id": "N5Vhr4_vECld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ptoQwAGEDQnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras import layers\n",
        "from keras.layers import Dropout, Conv1D, MaxPooling1D, Flatten, Dense, SpatialDropout1D, BatchNormalization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import schedules, Adam, Adadelta, SGD, RMSprop, Adagrad, Adamax, Nadam, Ftrl # Estamos usando só o Nadam\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix"
      ],
      "metadata": {
        "id": "OeOYrHW9Bxs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tweets_list):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(tweets_list)\n",
        "    return tokenizer.texts_to_sequences(tweets_list), tokenizer"
      ],
      "metadata": {
        "id": "6Uo-jZkFEH7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(text_tokenized, length=None):\n",
        "    return pad_sequences(text_tokenized, maxlen=length, padding='post')"
      ],
      "metadata": {
        "id": "1CAzhpVEEH7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(tweets_list, max_text_length):\n",
        "   \n",
        "    preprocess_tweets_list, tweets_list_tokenizer = tokenize(tweets_list)\n",
        "\n",
        "    preprocess_tweets_list = pad(preprocess_tweets_list, length=max_text_length)\n",
        "\n",
        "    return preprocess_tweets_list, tweets_list_tokenizer"
      ],
      "metadata": {
        "id": "hh2Kz-mBEH7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "inc5PzVAEH7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            self.score = roc_auc_score(self.y_val, y_pred)\n",
        "            self.false_positive_rate, self.true_positive_rate, _ = roc_curve(self.y_val, y_pred)\n",
        "            # print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, self.score))             "
      ],
      "metadata": {
        "id": "DLBaKxGhrbRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carrega arquivo de saída (resultados)"
      ],
      "metadata": {
        "id": "rbA3XJK6kIrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONSTANTES DOS RESULTADOS\n",
        "\n",
        "REDE = 'Bi-LSTM'\n",
        "\n",
        "BASE_0 = 'BASE 0'\n",
        "BASE_1 = 'BASE 1'\n",
        "BASE_2 = 'BASE 2'\n",
        "\n",
        "# Caminho arquivo de saída\n",
        "PATH_ARQ_SAIDA = \"/content/drive/MyDrive/TCC/resultados/resultados.csv\"\n",
        "\n",
        "#  rede base otimizador acuracia val_loss learning_rate dropout batch_size roc_curve train_resume confusion_matrix\n",
        "resultados = pd.read_csv(PATH_ARQ_SAIDA, index_col=0)\n",
        "resultados.head()"
      ],
      "metadata": {
        "id": "wvPJKDAQnAUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carrega base"
      ],
      "metadata": {
        "id": "RSH5BeEzEjL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base1 Com StopWords\n",
        "PATH_BASE1_JUNTO_COM_DA_LEILA_BALANCEADA = \"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base1JuntaHateENotHateDaLeila_balanceada.csv\"\n",
        "#MATRIZ_CBOW_300_BASE_1 = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base1JuntaHateENotHateDaLeila_balanceada_CBOW300.CSV\", delimiter=',')\n",
        "\n",
        "# Base2 Sem StopWords\n",
        "PATH_BASE2_LEILA_LIMPISSIMA_BALANCEADA = \"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base2_maior4_menor25_limpissima_balanceada.csv\"\n",
        "#MATRIZ_CBOW_300_BASE_2_LEILA = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base2_maior4_menor25_limpissima_balanceada_CBOW300.CSV\", delimiter=',')\n",
        "\n",
        "PATH_BASE_1_CLASSIFICADA_BALANCEADA = r\"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base1_classificada_balanceada.csv\" #entrada\n",
        "\n",
        "PATH_BASE_2_CLASSIFICADA_BALANCEADA = r\"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base2_classificada_balanceada.csv\" #entrada\n",
        "MATRIZ_CBOW_300_BASE_2 = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base2_classificada_balanceada_CBOW300.csv\", delimiter=',') #saida"
      ],
      "metadata": {
        "id": "us18Y81HEjL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execução modelo"
      ],
      "metadata": {
        "id": "W9pdFxE7Ml6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv(PATH_BASE_2_CLASSIFICADA_BALANCEADA, index_col=0)\n",
        "text_column = tweets['text']\n",
        "text_column"
      ],
      "metadata": {
        "id": "7wJXwK7jCkyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega a média de caracteres dos tweets de toda a base\n",
        "max_text_length = int(text_column.apply(lambda x: len(str(x).split(' '))).max())\n",
        "max_text_length"
      ],
      "metadata": {
        "id": "ud3ADezRSHtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxNdO2PW2E5a"
      },
      "outputs": [],
      "source": [
        "output_label = tweets['label']\n",
        "input_data, text_tokenizer = preprocess(text_column, None)\n",
        "    \n",
        "text_vocab_size = len(text_tokenizer.word_index)\n",
        "print(\"Vocabulary size:\", text_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parametros tunning:\n",
        "BATCH_SIZE = [32, 64]\n",
        "LEARN_RATE = [0.0001, 0.001]\n",
        "OTMIZADORES = ['Nadam', 'RMSProp', 'SGD']\n",
        "DROPOUT = [0.1, 0.2, 0.3]\n",
        "\n",
        "# parametros estaticos:\n",
        "EPOCHS = 10\n",
        "VAL_AND_TST_SIZE = 0.2 # Fazer na mão\n",
        "EMBEDDING_DIMENSION = 300  \n",
        "MAX_TEXT_SIZE = max_text_length \n",
        "VOCAB_SIZE = text_vocab_size"
      ],
      "metadata": {
        "id": "A8eO5V_FFfqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize (optimize, learning_rate):\n",
        "  if optimize == 'Nadam':\n",
        "    return Nadam(learning_rate=learning_rate, name=\"Nadam\")\n",
        "  \n",
        "  if optimize == 'RMSProp':\n",
        "    return RMSprop(learning_rate=learning_rate, name=\"RMSprop\")\n",
        "  \n",
        "  if optimize == 'SGD':\n",
        "    return SGD(learning_rate=learning_rate, name=\"SGD\")"
      ],
      "metadata": {
        "id": "QN3Vc_PWFkIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bilstm (vocab_size, embedding_dimen, max_text_size, dropout_rate ):\n",
        "  forward_layer = keras.layers.LSTM(max_text_size, return_sequences=False, dropout=dropout_rate)\n",
        "  backward_layer = keras.layers.LSTM(max_text_size, activation='sigmoid', return_sequences=False,\n",
        "                       go_backwards=True, dropout=dropout_rate)\n",
        "  \n",
        "  modelo = keras.Sequential([\n",
        "    keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION, input_length=max_text_size),\n",
        "    # keras.layers.Embedding(input_dim=MATRIZ_CBOW_300_BASE_2.shape[0],\n",
        "                            # output_dim=MATRIZ_CBOW_300_BASE_2.shape[1],\n",
        "                            # weights=[MATRIZ_CBOW_300_BASE_2],\n",
        "                            # embeddings_initializer=keras.initializers.Constant(MATRIZ_CBOW_300_BASE_2),\n",
        "                            # input_length=max_text_size,\n",
        "                            # trainable=True),\n",
        "    #keras.layers.Bidirectional(layers.LSTM(50, dropout=0.15, name=\"LSTM\")),\n",
        "    keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  return modelo  "
      ],
      "metadata": {
        "id": "wKH2mHM5r8_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(input_data, output_label, embedding_dimen, batch_size, epochs, validation_and_test_size, learning_rate, optimizer, dropout_rate):\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(input_data, output_label, test_size=validation_and_test_size, random_state=42)\n",
        "\n",
        "    opt = optimize(optimizer, learning_rate)\n",
        "    \n",
        "    model = create_bilstm(VOCAB_SIZE, EMBEDDING_DIMENSION, MAX_TEXT_SIZE, dropout_rate)\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=opt, metrics=['accuracy'])        \n",
        "    \n",
        "    ra_val = RocAucEvaluation(validation_data=(x_test, y_test), interval = 1)\n",
        "    \n",
        "    csv_logger = CSVLogger('log.csv', append=False, separator=';')                                                                                       \n",
        "    \n",
        "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split = validation_and_test_size, verbose=0, use_multiprocessing=True,  callbacks = [ra_val, csv_logger])\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "    \n",
        "    return history, model, x_test, y_test, ra_val, scores, csv_logger\n"
      ],
      "metadata": {
        "id": "ojOFJWxZH2Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history, model1, x_test, y_test, ra_val, scores, csv_logger = train_model(input_data, output_label, EMBEDDING_DIMENSION, 32, 5, 0.2, 0.001, 'Nadam', 0.2)\n",
        "# print(scores)"
      ],
      "metadata": {
        "id": "tMMKN1PS6L33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot dos gráficos\n",
        "for BS in BATCH_SIZE:\n",
        "  for LR in LEARN_RATE:\n",
        "    for OT in OTMIZADORES:\n",
        "      for DO in DROPOUT:\n",
        "        \n",
        "        history, model1, x_test, y_test, ra_val, scores, csv_logger = train_model(input_data, output_label, EMBEDDING_DIMENSION, BS, EPOCHS, VAL_AND_TST_SIZE, LR, OT, DO)\n",
        "\n",
        "        #\n",
        "        y_predito = model1.predict(x_test).round()\n",
        "        y_gabarito = y_test\n",
        "        \n",
        "        # Parâmetros\n",
        "        # rede base otimizador acuracia val_loss learning_rate dropout batch_size roc_curve train_resume confusion_matrix\n",
        "        NOME_REDE = REDE\n",
        "        NOME_BASE = BASE_2\n",
        "        NOME_OTIMIZADOR = OT\n",
        "        ACURACIA = scores[1]\n",
        "        VAL_LOSS = scores[0] \n",
        "        LEARNING_RATE = LR\n",
        "        DROPOUT_RATE = DO\n",
        "        BATCH = BS\n",
        "        ROC_CURVE = json.dumps({\n",
        "            \"false_positive_rate\": list(ra_val.false_positive_rate),\n",
        "            \"true_positive_rate\": list(ra_val.true_positive_rate),\n",
        "            \"score\": ra_val.score})\n",
        "        TRAIN_RESUME = json.dumps(pd.read_csv('log.csv',sep=';').to_dict()) \n",
        "        \n",
        "        confusion = confusion_matrix(y_gabarito, y_predito)\n",
        "        CONFUSION_MATRIX = json.dumps({\n",
        "            \"00\": int(confusion[0][0]),\n",
        "            \"01\": int(confusion[0][1]),\n",
        "            \"10\": int(confusion[1][0]),\n",
        "            \"11\": int(confusion[1][1])\n",
        "        })\n",
        " \n",
        "        # Salvando resultado do modelo\n",
        "        registro_resultado = {resultados.columns[0]: NOME_REDE, \n",
        "                              resultados.columns[1]: NOME_BASE, \n",
        "                              resultados.columns[2]: NOME_OTIMIZADOR,\n",
        "                              resultados.columns[3]: ACURACIA, \n",
        "                              resultados.columns[4]: VAL_LOSS,\n",
        "                              resultados.columns[5]: LEARNING_RATE,\n",
        "                              resultados.columns[6]: DROPOUT_RATE,\n",
        "                              resultados.columns[7]: BATCH,\n",
        "                              resultados.columns[8]: ROC_CURVE,\n",
        "                              resultados.columns[9]: TRAIN_RESUME,\n",
        "                              resultados.columns[10]: CONFUSION_MATRIX\n",
        "                            }\n",
        "\n",
        "        resultados = resultados.append(registro_resultado, ignore_index=True)"
      ],
      "metadata": {
        "id": "9YLL1kZTDzUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados"
      ],
      "metadata": {
        "id": "jXQefhfUIqiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caso queira deletar uma linha use o código abaixo\n",
        "# Use a propriedade label para especificar o índice da linha\n",
        "#resultados = resultados.drop(labels=1, axis=0)\n",
        "#resultados = resultados.reset_index(drop=True)\n",
        "#resultados"
      ],
      "metadata": {
        "id": "nP3enYmss2v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salva arquivo de saída\n",
        "resultados.to_csv(PATH_ARQ_SAIDA)"
      ],
      "metadata": {
        "id": "y519ohA8u9jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model1, to_file='/meuarquivo.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "SqzyjKSXJ6Tq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}