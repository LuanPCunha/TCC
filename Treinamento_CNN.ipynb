{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Treinamento_CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuanPCunha/TCC/blob/main/Treinamento_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurações Iniciais "
      ],
      "metadata": {
        "id": "uWzXvUMH86xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oj5I6fco9eB",
        "outputId": "3537575c-8406-4b12-eaab-17e27df2b2e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from numpy import loadtxt\n",
        "import string, re\n",
        "import itertools\n",
        "import nltk\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "py.init_notebook_mode(connected=True)\n",
        "%matplotlib inline\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YYGxUH68sUuH",
        "outputId": "0dac3767-9494-4b00-ec50-20d1df754e15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0hk9_ndbmUK"
      },
      "source": [
        "## Lendo a Base"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base1 Com StopWords\n",
        "PATH_BASE1 = \"/content/drive/MyDrive/TCC/dados/processadas/Base1_classificada.csv\"\n",
        "\n",
        "# Base2 Sem StopWords\n",
        "PATH_BASE2 = \"/content/drive/MyDrive/TCC/dados/processadas/Base2_classificada.csv\"\n",
        "\n",
        "# Base1 Com StopWords\n",
        "PATH_BASE1_JUNTO_COM_DA_LEILA_BALANCEADA = \"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base1JuntaHateENotHateDaLeila_balanceada.csv\"\n",
        "#MATRIZ_CBOW_300_BASE_1 = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base1JuntaHateENotHateDaLeila_balanceada_CBOW300.CSV\", delimiter=',')\n",
        "\n",
        "# Base2 Sem StopWords\n",
        "PATH_BASE2_LEILA_LIMPISSIMA_BALANCEADA = \"/content/drive/MyDrive/TCC/dados/processadas/balanceadas/Base2_maior4_menor25_limpissima_balanceada.csv\"\n",
        "#MATRIZ_CBOW_300_BASE_2_LEILA = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base2_maior4_menor25_limpissima_balanceada_CBOW300.CSV\", delimiter=',')\n",
        "\n",
        "PATH_BASE_2_CLASSIFICADA = r\"/content/drive/MyDrive/TCC/dados/processadas/Base2_classificada.csv\" #entrada\n",
        "MATRIZ_CBOW_300_BASE_2 = loadtxt(\"/content/drive/MyDrive/TCC/dados/word_embeddings/Matriz_Base2_classificada_CBOW300.csv\", delimiter=',') #saida"
      ],
      "metadata": {
        "id": "W6LWWnwVpLVQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MATRIZ_CBOW_300_BASE_2.shape"
      ],
      "metadata": {
        "id": "ngkr5ZHKH5ig",
        "outputId": "a7ae34fd-434b-44a7-ddb8-e0a11eaad21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50997, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YxMcfFGUMqoD"
      },
      "outputs": [],
      "source": [
        "#trainDF = pd.read_csv(PATH_BASE2_LEILA_LIMPISSIMA_BALANCEADA, index_col=0)\n",
        "trainDF = pd.read_csv(PATH_BASE_2_CLASSIFICADA, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "aiuYPDsqbeqQ",
        "outputId": "826e131b-50e1-4865-afd4-26b4ebf13242"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           text\n",
              "label          \n",
              "hate       5287\n",
              "not_hate  56351"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2dbea35c-07e4-4605-99ff-7e78251729ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <td>5287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not_hate</th>\n",
              "      <td>56351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dbea35c-07e4-4605-99ff-7e78251729ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2dbea35c-07e4-4605-99ff-7e78251729ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2dbea35c-07e4-4605-99ff-7e78251729ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Agrupa dados por label\n",
        "trainDF.groupby('label').count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega a média de caracteres dos tweets de toda a base\n",
        "average_len = int(trainDF['text'].apply(lambda x: len(str(x).split(' '))).mean())\n",
        "average_len"
      ],
      "metadata": {
        "id": "QN8zzra54iMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega a média de caracteres dos tweets de toda a base\n",
        "max_len = int(trainDF['text'].apply(lambda x: len(str(x).split(' '))).max())\n",
        "max_len"
      ],
      "metadata": {
        "id": "5ZWXjrkH1HK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palavras_unicas = list ( trainDF['text'].str.split(' ', expand=True).stack().unique() )\n",
        "palavras_unicas = set(map(lambda x: re.findall(r'^\\D+', x)[0] if re.findall(r'^\\D+', x) != [] else \"a\" ,palavras_unicas)) \n",
        "quantidade_palavras_unicas = len( palavras_unicas )\n",
        "quantidade_palavras_unicas"
      ],
      "metadata": {
        "id": "UhOYgOU8EN6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDF['label'].astype(int)\n",
        "print(type(trainDF['label'].astype(int)[0]))\n",
        "print(type(trainDF['label'][0]))"
      ],
      "metadata": {
        "id": "2a04Qu0KNPCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF (NÃO CONSEGUIMOS USAR NO MODELO CNN)"
      ],
      "metadata": {
        "id": "wmhNqURK5vPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #word level tf-idf\n",
        "# #td-idf gera matriz com 9140 colunas (4 palavras estão sendo perdidas ao usar o regex \\w{1,})\n",
        "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features= quantidade_palavras_unicas)\n",
        "# X = tfidf_vect.fit_transform(trainDF['text'])\n",
        "\n",
        "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(X, trainDF['label'], test_size=0.3, random_state=2)\n",
        "# # Não conseguimos usar o c-bow300"
      ],
      "metadata": {
        "id": "8uLpWFP05veb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X.shape"
      ],
      "metadata": {
        "id": "0RTt7nU7G16H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "train_x_text, valid_x_text, train_y_text, valid_y_text = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.3, random_state=2)"
      ],
      "metadata": {
        "id": "JZqSl5-fzg5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(word_index) + 1"
      ],
      "metadata": {
        "id": "qidvB_bjGHD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x_text), maxlen=max_len)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x_text), maxlen=max_len)"
      ],
      "metadata": {
        "id": "Xy5zX1RIzZ9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X.shape[1])\n"
      ],
      "metadata": {
        "id": "OY0B7cnA9woa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRJ3Z8M3jGRs"
      },
      "source": [
        "# CNN model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_cnn(input_size):\n",
        "#     # Add an Input Layer\n",
        "#     input_layer = layers.Input((input_size,1,))\n",
        "\n",
        "#     middle_layer = layers.Dropout(0.3)(input_layer)\n",
        "    \n",
        "#     # middle_layer = layers.Convolution1D(100, 3, activation=\"relu\")(middle_layer)\n",
        "#     # middle_layer = layers.Convolution1D(100, 3, activation=\"relu\")(middle_layer)\n",
        "#     # middle_layer = layers.Dropout(0.3)(middle_layer)\n",
        "#     # middle_layer = layers.MaxPooling1D()(middle_layer)\n",
        "    \n",
        "#     middle_layer = layers.Convolution1D(100, 3, activation=\"relu\")(middle_layer)\n",
        "#     middle_layer = layers.Convolution1D(100, 3, activation=\"relu\")(middle_layer)\n",
        "#     middle_layer = layers.Dropout(0.3)(middle_layer)\n",
        "#     middle_layer = layers.MaxPooling1D()(middle_layer)\n",
        "    \n",
        "#     middle_layer = layers.Flatten()(middle_layer)\n",
        "#     output_layer = layers.Dense(64, activation=\"relu\")(middle_layer)\n",
        "\n",
        "#     # Compile the model\n",
        "#     model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['acc', 'mae'])\n",
        "  \n",
        "#     return model"
      ],
      "metadata": {
        "id": "lGjZVTt-GGiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHk31johw4MA"
      },
      "outputs": [],
      "source": [
        "# def create_cnn(input_size):\n",
        "#     # Add an Input Layer\n",
        "#     input_layer = layers.Input((input_size,1,))\n",
        "\n",
        "#     input_layer = layers.SpatialDropout1D(0.3)(input_layer)\n",
        "\n",
        "#     # Add the word embedding Layer\n",
        "#        # embedding_layer = layers.Embedding(X.shape[1] + 1, 300, weights=[MATRIZ_CBOW_300_BASE_2], trainable=True)(input_layer)\n",
        "#     embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "#     # Add the convolutional Layer\n",
        "#     #conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(input_layer)\n",
        "#     conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "#     # Add the pooling Layer\n",
        "#     pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "#     # Add the output Layers\n",
        "#     output_layer1 = layers.Dense(64, activation=\"relu\")(pooling_layer)\n",
        "#     output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "#     output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "#     # Compile the model\n",
        "#     model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['acc', 'mae'])\n",
        "  \n",
        "#     return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, validation, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    print(\"aqui1\")\n",
        "    classifier.fit(feature_vector_train, label, epochs=15) #adicionamos epoca\n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    print(\"aqui2\")\n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    print(\"aqui3\")\n",
        "    return metrics.accuracy_score(predictions, validation)"
      ],
      "metadata": {
        "id": "oinV3GGN22R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(9085,\n",
        "                           300,\n",
        "                            weights=[MATRIZ_CBOW_300_BASE_2],\n",
        "                            input_length=max_len,\n",
        "                            trainable=False)"
      ],
      "metadata": {
        "id": "8s708ZBWFdvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn(input_size):\n",
        "    sequence_input = layers.Input(shape=(input_size,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    x = layers.Conv1D(128, 1, activation='relu',name=\"COVID\")(embedded_sequences)\n",
        "    x = layers.MaxPooling1D(1)(x)\n",
        "    x = layers.Conv1D(128, 1, activation='relu')(x)\n",
        "    x = layers.MaxPooling1D(1)(x)\n",
        "    x = layers.Conv1D(128, 1, activation='relu')(x)\n",
        "    x = layers.MaxPooling1D(25)(x)  # global max pooling\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    preds = layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "    # model = models.Model(sequence_input, preds)\n",
        "    # model.compile(loss='categorical_crossentropy',\n",
        "    #               optimizer='rmsprop',\n",
        "    #               metrics=['acc'])\n",
        "    \n",
        "\n",
        "    #  Compile the model\n",
        "    model = models.Model(inputs=sequence_input, outputs=preds)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['acc', 'mae'])\n",
        "  \n",
        "    return model"
      ],
      "metadata": {
        "id": "o7GMML4ZE62O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ODtGE3xw3H"
      },
      "outputs": [],
      "source": [
        "classifier = create_cnn(max_len)\n",
        "accuracyCNN = train_model(classifier,train_seq_x, train_y_text, valid_seq_x, valid_y_text, is_neural_net=True)\n",
        "\n",
        "print (\"CNN, Word Embeddings - accuracy\",  accuracyCNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "\n",
        "\n",
        "\n",
        "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "\n",
        "\n",
        "Fazer testes com as Bases BALANCEADAS"
      ],
      "metadata": {
        "id": "dU6cqtBSOXm9"
      }
    }
  ]
}